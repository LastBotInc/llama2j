---------------- llama2j ----------------
CUDA llama2_7b.bin 2 GPU
MAX_THREADS_PER_BLOCK = 32;
achieved tok/s: 9.8

Parallelize
// qkv matmuls for this position
achieved tok/s: 10.0

Attention in two blocks
achieved tok/s: 10.1

parallelize attention
achieved tok/s: 10.2

eliminate redundant cuda.memZeroFloat.call(0, tmp1CU, 0, 1);
achieved tok/s: 10.3

MAX_THREADS_PER_BLOCK = 64;
achieved tok/s: 9.2


CPU llama2_7b.bin
achieved tok/s: 4.7


---------------- llama2.c ----------------
#make runomp

./run stories15M.bin -i "One day, Lily met a Shoggoth"
achieved tok/s: 1235.3

./run stories42M.bin -i "One day, Lily met a Shoggoth"
achieved tok/s: 843.0

MP_NUM_THREADS=8 ./run llama2_7b.bin -t 1.0 -n 256 -i "One day, Lily met a Shoggoth"
achieved tok/s: 11.2

---------------- llama2j ----------------

./run stories15M.bin
achieved tok/s: 255.0 (20.6%)

./run stories42M.bin
achieved tok/s: 151.7 (18.0%)

THREAD_COUNT = 32;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 4.7 (42%)

--- OLD ---

THREAD_COUNT = 8;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 1.5 (13.4%)

THREAD_COUNT = 16;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 2.8 (17.9%)

THREAD_COUNT = 32;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 4.6 (41%)

THREAD_COUNT = 64;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 2.7 (24.1%)

----------------
