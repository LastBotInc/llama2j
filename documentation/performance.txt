------------ CUDA llama2_7b_chat.bin 1 GPU ------------
achieved tok/s: 4.3

eliminate memZeroFloat()
achieved tok/s: 4.28

eliminate all memZeroFloat()
achieved tok/s: 4.30

FindMax:
BLOCK_SIZE = 256;
SIMPLE_KERNEL_THRESHOLD = 8
achieved tok/s: 4.28

BLOCK_SIZE = 256;
SIMPLE_KERNEL_THRESHOLD = 128;
achieved tok/s: 4.28

BLOCK_SIZE = 64;
SIMPLE_KERNEL_THRESHOLD = 128;
achieved tok/s: 4.28

BLOCK_SIZE = 64;
SIMPLE_KERNEL_THRESHOLD = 8;
achieved tok/s: 4.29

BLOCK_SIZE = 32;
SIMPLE_KERNEL_THRESHOLD = 8;
achieved tok/s: 4.28

BLOCK_SIZE = 128;
SIMPLE_KERNEL_THRESHOLD = 8;
achieved tok/s: 4.28

BLOCK_SIZE = 512;
SIMPLE_KERNEL_THRESHOLD = 8;
achieved tok/s: 4.28

BLOCK_SIZE = 64;
SIMPLE_KERNEL_THRESHOLD = 32;
achieved tok/s: 4.29

QUANT_GROUP_SIZE = 32;
achieved tok/s: 4.29

QUANT_GROUP_SIZE = 64;
achieved tok/s: 4.91

QUANT_GROUP_SIZE = 128;
achieved tok/s: 5.28

QUANT_GROUP_SIZE = 512;
achieved tok/s: 5.76

QUANT_GROUP_SIZE = 1024;
achieved tok/s: 5.79

QUANT_GROUP_SIZE = 256;
achieved tok/s: 5.43

ExpAndSum
BLOCK_SIZE = 32;
achieved tok/s: 5.42

BLOCK_SIZE = 64;
achieved tok/s: 5.43

BLOCK_SIZE = 128;
achieved tok/s: 5.45

BLOCK_SIZE = 256;
achieved tok/s: 5.47

BLOCK_SIZE = 512;
achieved tok/s: 5.46

BLOCK_SIZE = 1024;
achieved tok/s: 5.46

->
BLOCK_SIZE = 128;
achieved tok/s: 5.45

CPU
AccumWeightedValue unroll 4
achieved tok/s: 2.78

AccumWeightedValue unroll 8
achieved tok/s: 2.70

AccumWeightedValue no unroll
achieved tok/s: 2.75

run setup.sh
achieved tok/s: 2.77

baseline
achieved tok/s: 5.45

attentionLoop to store also max
achieved tok/s: 5.49

combine normalize to ExpSum
achieved tok/s: 5.49

- updated all
ExpSumNormalize BLOCK_SIZE = 16;
achieved tok/s: 5.49

remove unnecessary syncthreads
SumOfSquares BLOCK_SIZE = 16;
achieved tok/s: 5.48

SumOfSquares BLOCK_SIZE = 128;
achieved tok/s: 5.54

SumOfSquares BLOCK_SIZE = 1024;
achieved tok/s: 5.54

SumOfSquares BLOCK_SIZE = 256;
achieved tok/s: 5.54

Attention parallel
achieved tok/s: 5.22

// non-parallel
SumOfSquares BLOCK_SIZE = 256;
achieved tok/s: 5.57

Calculational rope



----- CPU----- CPU----- CPU----- CPU

dynamic threadcount
achieved tok/s: 5.25

---------------- llama2j ----------------
CUDA llama2_7b.bin 2 GPU
MAX_THREADS_PER_BLOCK = 32;
achieved tok/s: 9.8

Parallelize
// qkv matmuls for this position
achieved tok/s: 10.0

Attention in two blocks
achieved tok/s: 10.1

parallelize attention
achieved tok/s: 10.2

eliminate redundant cuda.memZeroFloat.call(0, tmp1CU, 0, 1);
achieved tok/s: 10.3

MAX_THREADS_PER_BLOCK = 64;
achieved tok/s: 9.2

CPU llama2_7b.bin
achieved tok/s: 4.7




---------------- llama2.c ----------------
#make runomp

./run stories15M.bin -i "One day, Lily met a Shoggoth"
achieved tok/s: 1235.3

./run stories42M.bin -i "One day, Lily met a Shoggoth"
achieved tok/s: 843.0

MP_NUM_THREADS=8 ./run llama2_7b.bin -t 1.0 -n 256 -i "One day, Lily met a Shoggoth"
achieved tok/s: 11.2

---------------- llama2j ----------------

./run stories15M.bin
achieved tok/s: 255.0 (20.6%)

./run stories42M.bin
achieved tok/s: 151.7 (18.0%)

THREAD_COUNT = 32;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 4.7 (42%)

--- OLD ---

THREAD_COUNT = 8;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 1.5 (13.4%)

THREAD_COUNT = 16;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 2.8 (17.9%)

THREAD_COUNT = 32;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 4.6 (41%)

THREAD_COUNT = 64;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 2.7 (24.1%)

----------------
