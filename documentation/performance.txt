------------ CUDA llama2_7b_chat.bin 1 GPU ------------
achieved tok/s: 4.3

eliminate memZeroFloat()
achieved tok/s: 4.28

eliminate all memZeroFloat()
achieved tok/s: 4.30

FindMax:
BLOCK_SIZE = 256;
SIMPLE_KERNEL_THRESHOLD = 8
achieved tok/s: 4.28

BLOCK_SIZE = 256;
SIMPLE_KERNEL_THRESHOLD = 128;
achieved tok/s: 4.28

BLOCK_SIZE = 64;
SIMPLE_KERNEL_THRESHOLD = 128;
achieved tok/s: 4.28

BLOCK_SIZE = 64;
SIMPLE_KERNEL_THRESHOLD = 8;
achieved tok/s: 4.29

BLOCK_SIZE = 32;
SIMPLE_KERNEL_THRESHOLD = 8;
achieved tok/s: 4.28

BLOCK_SIZE = 128;
SIMPLE_KERNEL_THRESHOLD = 8;
achieved tok/s: 4.28

BLOCK_SIZE = 512;
SIMPLE_KERNEL_THRESHOLD = 8;
achieved tok/s: 4.28

BLOCK_SIZE = 64;
SIMPLE_KERNEL_THRESHOLD = 32;
achieved tok/s: 4.29

QUANT_GROUP_SIZE = 32;
achieved tok/s: 4.29

QUANT_GROUP_SIZE = 64;
achieved tok/s: 4.91

QUANT_GROUP_SIZE = 128;
achieved tok/s: 5.28

QUANT_GROUP_SIZE = 512;
achieved tok/s: 5.76

QUANT_GROUP_SIZE = 1024;
achieved tok/s: 5.79

QUANT_GROUP_SIZE = 256;
achieved tok/s: 5.43

ExpAndSum
BLOCK_SIZE = 32;
achieved tok/s: 5.42

BLOCK_SIZE = 64;
achieved tok/s: 5.43

BLOCK_SIZE = 128;
achieved tok/s: 5.45

BLOCK_SIZE = 256;
achieved tok/s: 5.47

BLOCK_SIZE = 512;
achieved tok/s: 5.46

BLOCK_SIZE = 1024;
achieved tok/s: 5.46

->
BLOCK_SIZE = 128;
achieved tok/s: 5.45

CPU
AccumWeightedValue unroll 4
achieved tok/s: 2.78

AccumWeightedValue unroll 8
achieved tok/s: 2.70

AccumWeightedValue no unroll
achieved tok/s: 2.75

run setup.sh
achieved tok/s: 2.77

baseline
achieved tok/s: 5.45

attentionLoop to store also max
achieved tok/s: 5.49

combine normalize to ExpSum
achieved tok/s: 5.49

- updated all
ExpSumNormalize BLOCK_SIZE = 16;
achieved tok/s: 5.49

remove unnecessary syncthreads
SumOfSquares BLOCK_SIZE = 16;
achieved tok/s: 5.48

SumOfSquares BLOCK_SIZE = 128;
achieved tok/s: 5.54

SumOfSquares BLOCK_SIZE = 1024;
achieved tok/s: 5.54

SumOfSquares BLOCK_SIZE = 256;
achieved tok/s: 5.54

Attention parallel
achieved tok/s: 5.22

// non-parallel
SumOfSquares BLOCK_SIZE = 256;
achieved tok/s: 5.57

Optimize MatMul
achieved tok/s: 8.97

Further Optimize MatMul
achieved tok/s: 9.17

Optimize groupPayloadBase
achieved tok/s: 8.78
-> revert

without static cast
8.97

eliminate groupPayloadBase
achieved tok/s: 9.17

End condition to loop
achieved tok/s: 11.77

achieved tok/s: 11.71

loop start and end
achieved tok/s: 12.83, 12.84, 12.81

Union byte to float conversion
achieved tok/s: 12.83

#pragma unroll 48
achieved tok/s: 13.12

#pragma unroll 24
achieved tok/s: 13.08

#pragma unroll 32
achieved tok/s: 13.26

#pragma unroll 16
achieved tok/s: 13.20

#pragma unroll 8
achieved tok/s: 12.55


#pragma unroll 32
BLOCK_SIZE = 64;
achieved tok/s: 13.26

BLOCK_SIZE = 32;
achieved tok/s: 14.30

BLOCK_SIZE = 16;
achieved tok/s: 15.38

BLOCK_SIZE = 8;
achieved tok/s: 16.06

BLOCK_SIZE = 4;
achieved tok/s: 16.29

BLOCK_SIZE = 2;
achieved tok/s: 14.68

BLOCK_SIZE = 1;
achieved tok/s: 9.05

BLOCK_SIZE = 128;
achieved tok/s:11.95

BLOCK_SIZE = 256;
achieved tok/s:

MatMul BLOCK_SIZE = 4;
achieved tok/s: 16.32

Parallelize qkv matmuls
achieved tok/s: 16.63

Parallelize qkv matmuls better
achieved tok/s: 16.65

Parallelize store kv cache
achieved tok/s: 16.69

Parallelize heads
achieved tok/s: 17.75

h % (ContextCUDA.STREAM_COUNT / 2);
achieved tok/s: 17.77

h % 8;
achieved tok/s: 17.73

h % 4;
achieved tok/s: 17.73

h % 2;
achieved tok/s: 17.74

h % 1;
achieved tok/s: 17.74

h % 16;
achieved tok/s: 17.80, 17.66

parallelize self.w1(x)), self.w3(x)
achieved tok/s: 17.88, 17.86, 17.72, 17.59

accumWeightedValue, #pragma unroll 64
achieved tok/s: 17.80

accumWeightedValue, #pragma unroll 32
achieved tok/s: 17.67

accumWeightedValue, #pragma unroll 16
achieved tok/s: 17.82

accumWeightedValue, #pragma unroll 8
achieved tok/s:17.86, 17.79

accumWeightedValue, #pragma unroll 4
achieved tok/s:17.72

accumWeightedValue optimization
#pragma unroll 8
achieved tok/s: 17.84

#pragma unroll 4
achieved tok/s: 17.81

#pragma unroll 16
achieved tok/s: 17.84

AccumWeightedValue
BLOCK_SIZE = 64;
achieved tok/s: 17.84, 17/81

BLOCK_SIZE = 32;
achieved tok/s: 17.74

BLOCK_SIZE = 16;
achieved tok/s: 17.73

BLOCK_SIZE = 128;
achieved tok/s: 17.84, 17.78


BLOCK_SIZE = 256;
achieved tok/s: 17.82

BLOCK_SIZE = 64;
17.81

AttentionLoop, optimize
achieved tok/s: 17.86,17.76

AttentionLoop
#pragma unroll 16
achieved tok/s: 17.87

#pragma unroll 8
achieved tok/s: 17.78

#pragma unroll 4
achieved tok/s: 17.71

#pragma unroll 2
achieved tok/s: 17.84

#pragma unroll 32

AttentionLoop
BLOCK_SIZE = dynamic;
achieved tok/s: 17.87

AttentionLoop

BLOCK_SIZE = 1;
achieved tok/s: 17.75

BLOCK_SIZE = 2;
achieved tok/s: 17.88, 17.78

BLOCK_SIZE = 4;
achieved tok/s: 17.86

BLOCK_SIZE = 8;
achieved tok/s: 17.82

BLOCK_SIZE = 16;
achieved tok/s: 17.81

BLOCK_SIZE = 32;
achieved tok/s: 17.87

BLOCK_SIZE = 64;
achieved tok/s: 17.85

BLOCK_SIZE = 128;
achieved tok/s: 17.76

BLOCK_SIZE = 256;
achieved tok/s: 17.74

BLOCK_SIZE = 512;
achieved tok/s: 17.73

baseline


SumOfSquares
achieved tok/s: 17.70, 17.79,

sumOfSquares loop optimization
achieved tok/s: 17.83, 17.78

#pragma unroll 32
achieved tok/s: 17.71

#pragma unroll 4
achieved tok/s: 17.79

#pragma unroll 2
achieved tok/s: 17.86

#pragma unroll 16
achieved tok/s: 17.87

#pragma unroll 8
BLOCK_SIZE = 256;
achieved tok/s: 17.85,

BLOCK_SIZE = 32;
achieved tok/s: 17.72

BLOCK_SIZE = 128;
achieved tok/s: 17.79

BLOCK_SIZE = 512;
achieved tok/s: 17.89, 17.83, 17.81

BLOCK_SIZE = 1024;
achieved tok/s:

baseline 17.85

optimize AccumWeightedValue
achieved tok/s: 17.86

--use_fast_math
achieved tok/s: 18.48

optimize AccumWeightedValue

#pragma unroll 1
achieved tok/s: 18.63

#pragma unroll 4
achieved tok/s: 18.63

#pragma unroll 8
achieved tok/s: 18.65

#pragma unroll 16
achieved tok/s: 18.68

#pragma unroll 32
achieved tok/s: 18.52

#pragma unroll 64
achieved tok/s: 18.51

#pragma unroll
achieved tok/s:

#pragma unroll 2
achieved tok/s: 18.67, 18.67

final loop if
achieved tok/s: 18.69

#pragma unroll 1
achieved tok/s: 18.70

#pragma unroll 2
achieved tok/s: 18.66

#pragma unroll 4
achieved tok/s: 18.69

#pragma unroll 8
achieved tok/s: 18.51

#pragma unroll 16
achieved tok/s: 18.67

#pragma unroll 32

expAndSumSmall, optim
achieved tok/s: 18.70

BLOCK_SIZE = 128;
achieved tok/s: 18.70

BLOCK_SIZE = 64;
achieved tok/s: 18.69

BLOCK_SIZE = 256;
achieved tok/s: 18.48

BLOCK_SIZE = 32;
achieved tok/s: 18.68

BLOCK_SIZE = 16;
achieved tok/s:

BLOCK_SIZE = 128;
achieved tok/s: 18.70

"--maxrregcount=" + "24",
achieved tok/s: 17.08

"--maxrregcount=" + "32",
achieved tok/s: 17.48

"--maxrregcount=" + "48",
achieved tok/s: 18.73

"--maxrregcount=" + "49",
achieved tok/s: 19.35

"--maxrregcount=" + "50",
achieved tok/s: 19.60

"--maxrregcount=" + "52",
achieved tok/s: 19.54

"--maxrregcount=" + "56",
achieved tok/s: 19.36

"--maxrregcount=" + "64",
achieved tok/s: 19.52

"--maxrregcount=" + "92",
achieved tok/s: 19.37

"--maxrregcount=" + "128",
achieved tok/s: 19.27

"--maxrregcount=" + "255",
achieved tok/s: 19.39


"--maxrregcount=" + "50",
achieved tok/s: 19.58

"--extra-device-vectorization",
achieved tok/s: 19.64
achieved tok/s: 19.60
achieved tok/s: 19.63

baseline
achieved tok/s: 19.58


achieved tok/s: 19.53
achieved tok/s: 19.43


"--ptxas-options=" + "-O3",
"--ptxas-options=" + "--allow-expensive-optimizations=true",

achieved tok/s: 19.45

attentionloop force inline
achieved tok/s: 19.40
achieved tok/s: 19.57
achieved tok/s: 19.58
achieved tok/s: 19.59

run heads in parallel
achieved tok/s: 12.68

int numberOfThreads = 1;
achieved tok/s: 14.09

int numberOfThreads = 2;
achieved tok/s: 15.09

int numberOfThreads = 4;
achieved tok/s:

int numberOfThreads = 8;
achieved tok/s: 13.44

int numberOfThreads = 32;
achieved tok/s: 12.92

----- CPU----- CPU----- CPU----- CPU

dynamic threadcount
achieved tok/s: 5.25

---------------- llama2j ----------------
CUDA llama2_7b.bin 2 GPU
MAX_THREADS_PER_BLOCK = 32;
achieved tok/s: 9.8

Parallelize
// qkv matmuls for this position
achieved tok/s: 10.0

Attention in two blocks
achieved tok/s: 10.1

parallelize attention
achieved tok/s: 10.2

eliminate redundant cuda.memZeroFloat.call(0, tmp1CU, 0, 1);
achieved tok/s: 10.3

MAX_THREADS_PER_BLOCK = 64;
achieved tok/s: 9.2

CPU llama2_7b.bin
achieved tok/s: 4.7




---------------- llama2.c ----------------
#make runomp

./run stories15M.bin -i "One day, Lily met a Shoggoth"
achieved tok/s: 1235.3

./run stories42M.bin -i "One day, Lily met a Shoggoth"
achieved tok/s: 843.0

MP_NUM_THREADS=8 ./run llama2_7b.bin -t 1.0 -n 256 -i "One day, Lily met a Shoggoth"
achieved tok/s: 11.2

---------------- llama2j ----------------

./run stories15M.bin
achieved tok/s: 255.0 (20.6%)

./run stories42M.bin
achieved tok/s: 151.7 (18.0%)

THREAD_COUNT = 32;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 4.7 (42%)

--- OLD ---

THREAD_COUNT = 8;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 1.5 (13.4%)

THREAD_COUNT = 16;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 2.8 (17.9%)

THREAD_COUNT = 32;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 4.6 (41%)

THREAD_COUNT = 64;
--checkpoint llama2_7b.bin --gpuMem "17,24,24,24"
achieved tok/s: 2.7 (24.1%)

----------------
